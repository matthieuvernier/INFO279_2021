{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding y Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivación: representar las dimensiones semánticas de cada palabra\n",
    "\n",
    "1. I want an orange.\n",
    "2. I wantrepresentar an apple.\n",
    "\n",
    "- Los enfoques <i>bag of words</i> no tienen la capacidad de calcular que las frases 1 y 2 son muy similares porque no tienen una manera de representar que las palabras 'orange' y 'apple' comparten caracterícas (<i>features</i>) comunes.\n",
    "\n",
    "Los enfoques ingenuos tieden a representar las palabras como vectores \"1-Hot\". Por ejemplo, supongamos que tenemos un vocabulario de sólo cinco palabras: King, Queen, Man, Woman y Child. Se codificaría la palabra 'Queen' como:\n",
    "\n",
    "<img src=\"img/word2vec1.png\"/>\n",
    "\n",
    "- Sería más interesante poder representar la semántica de cada palabra tomando en cuentas ciertas características. \n",
    "\n",
    "<img src=\"img/word2vec2.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición\n",
    "\n",
    "El concepto de **word embedding** se refiere a un conjunto de técnicas utilizadas para aprender representaciones matemáticas, tipicamente vectores, de cada palabra.\n",
    "\n",
    "Una de las técnicas más populares es __Word2Vec__ propuesto por un equipo de investigación de Google en 2013 (Efficient Estimation of Word Representations in Vector Space [Mikolov et al., 2013]).\n",
    "\n",
    "Alternativas populares son __GloVe__ (propuesta por la Universidad de Stanford en 2014) y __FastText__ (propuesta por Facebook en 2016), que extende Word2Vec para considerar de mejor manera las palabras con errores ortográficas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algunas propiedades de los word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tener representaciones vectoriales de las palabras permite calcular \"razonamiento\" de tipo __King - Man + Woman = ?__ y llegar a un resultado cerca de __Queen__.\n",
    "\n",
    "<img src=\"img/word2vec4.png\"/>\n",
    "\n",
    "- Tener representaciones vectoriales de las palabras permite realizar razonamientos analógicos de tipo __A es a B, lo que C es a ..__ . Este tipo de propiedades es muy útil para aplicaciones de _Question Answering_ por ejemplo. Las respuestas a las pregutas siguientes <i>¿Cuál es la capital de Chile?</i> o <i>¿Cuáles son los clubs de fútbol en Chile?</i> se pueden responder adicionando vectores.\n",
    "\n",
    "<img src=\"img/word2vec6.png\"/>\n",
    "\n",
    "<img src=\"img/word2vec7.png\"/>\n",
    "\n",
    "<img src=\"img/word2vec8.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo se aprenden los vectores? - Redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ara construir sus vectores, Word2Vec utiliza un dataset de entrenamiento y algoritmos de aprendizaje basados en redes neuronales (__Continuous Bag of Words__ (CBOW), o modelo __Skip Gram__). El objetivo de esta fase de aprendizaje es aprender cuáles son las palabras _X_ más probables de aparecer en el contexto de una palabra _y_.\n",
    "\n",
    "<img src=\"img/word2vec5.png\"/>\n",
    "\n",
    "Por ejemplo, ¿cuál es la probabilidad de tener la palabra 'perro' si aparece la palabra 'pelota' en el contexto?\n",
    "\n",
    "<code>Los expertos explican que los __perros__ persiguen __pelotas__ en movimiento como parte de un comportamiento instintivo. Aunque no todos los perros tienen tan despiertos su instinto de caza, esto no impide que la mayoría de ellos sí disfruten, y mucho, de los juegos que incluyen persecuciones de una saltarina __pelota__ que bota delante de ellos. </code>\n",
    "\n",
    "__Algoritmo CBOW__\n",
    "\n",
    "Las palabras de contexto forman la capa de entrada. Si el tamaño del vocabulario es V, estos serán vectores de dimensión V con sólo uno de los elementos establecido en uno, y el resto todos los ceros. Hay una sola capa oculta y una capa de salida.\n",
    "\n",
    "<img src=\"img/word2vec9.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un ejemplo práctico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase <code>word2vec</code> de Gensim permite word embeddings de palabras (ver documentación: https://radimrehurek.com/gensim/models/word2vec.html).\n",
    "\n",
    "Esta clase tiene varios parametros, en particular:\n",
    "- <code>sentences</code>: una lista de palabras o de frases que sirve para entrenar el modelo\n",
    "- <code>sg</code>: define que algoritmos de aprendizaje utilizar (0=CBOW, 1=skip-gram)\n",
    "- <code>vector_size</code>: define la dimensión de los vectores que se desea extraer\n",
    "- <code>window</code>: define el número de palabras considerar a la izquierda y a la derecha de una palabra\n",
    "- <code>min_count</code>: ignorar las palabras que aparecen menos de _min_count_\n",
    "y otros asociados a la parametrización de la fase de aprendizaje de la red neuronal (que no detallaremos en esta parte del curso):\n",
    "- <code>alpha</code>: el _learning rate_ utilizado para optimizar los parametros de la red neuronal.\n",
    "- <code>iter</code>: número de iteraciones (epocas) sobre el dataset para encontrar los parametreos que optimizan la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar nuestro modelo Word2Vec, podemos utilizar nuestros propios datasets o utilizar datasets genericos existentes. Para empezar, utilizaremos 100 MB de textos extraidos de Wikipedia en inglés, para generar vectores de 200 dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = word2vec.Text8Corpus('../datasets/text8.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(sentences,vector_size=200,hs=1)\n",
    "#model=word2vec.Word2Vec.load(\"text8_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=71290, vector_size=200, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que hemos aprendido nuestro modelo, tratemos de resolver la ecuación <code>King - Man + Woman</code>.\n",
    "\n",
    "En otras palabras buscamos cuál es el vector más similar al vector que adiciona positivamente 'King' y 'Woman' y negativamente 'Man'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.5515807867050171),\n",
       " ('throne', 0.4885711073875427),\n",
       " ('regent', 0.487594872713089),\n",
       " ('monarch', 0.479781836271286),\n",
       " ('prince', 0.4787699282169342)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['woman','king'],negative=['man'],topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('conflicts', 0.6832011342048645),\n",
       " ('clashes', 0.667018473148346),\n",
       " ('hostilities', 0.6493234634399414),\n",
       " ('tensions', 0.6465848684310913),\n",
       " ('confrontation', 0.6254630088806152),\n",
       " ('struggle', 0.6226808428764343),\n",
       " ('confrontations', 0.6206727623939514),\n",
       " ('strife', 0.6187384724617004),\n",
       " ('disagreements', 0.6130479574203491),\n",
       " ('dispute', 0.5851774215698242)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"conflict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('confrontation', 0.5802947282791138),\n",
       " ('weapons', 0.5498438477516174),\n",
       " ('warfare', 0.5303004384040833),\n",
       " ('rifle', 0.5301260948181152),\n",
       " ('struggle', 0.5106883645057678),\n",
       " ('confrontations', 0.5069210529327393),\n",
       " ('pistol', 0.5058635473251343),\n",
       " ('fighting', 0.4914857745170593),\n",
       " ('ammunition', 0.48768478631973267),\n",
       " ('conflicts', 0.48271921277046204)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"conflict\",\"weapon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('disagreements', 0.49566638469696045),\n",
       " ('clashes', 0.4933446943759918),\n",
       " ('conflicts', 0.4849163293838501),\n",
       " ('disputes', 0.45843130350112915),\n",
       " ('tensions', 0.4419403374195099),\n",
       " ('hostilities', 0.4400283694267273),\n",
       " ('dispute', 0.4245198667049408),\n",
       " ('strife', 0.4076556861400604),\n",
       " ('infighting', 0.3949545621871948),\n",
       " ('antagonism', 0.39344874024391174)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"conflict\"],negative=[\"weapon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('childhood', 0.5314176678657532),\n",
       " ('career', 0.5136758685112),\n",
       " ('lives', 0.5071121454238892),\n",
       " ('experiences', 0.4583725035190582),\n",
       " ('humanity', 0.4511985182762146),\n",
       " ('work', 0.4490172266960144),\n",
       " ('mankind', 0.43727898597717285),\n",
       " ('universe', 0.4271804094314575),\n",
       " ('teens', 0.4086006283760071),\n",
       " ('happiness', 0.40494653582572937)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"life\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('childhood', 0.3892081081867218),\n",
       " ('adolescence', 0.3318144679069519),\n",
       " ('experiences', 0.330782413482666),\n",
       " ('spiritual', 0.32153668999671936),\n",
       " ('career', 0.3140907287597656),\n",
       " ('stillness', 0.3087413012981415),\n",
       " ('ueshiba', 0.30592378973960876),\n",
       " ('biography', 0.3013848662376404),\n",
       " ('reptilian', 0.29709941148757935),\n",
       " ('rebirth', 0.2940960228443146)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"life\"],negative=[\"money\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver los parametros aprendidos por la red neuronal para una palabra dada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.3508972 , -1.0290902 ,  0.97134167, -1.3990436 ,  0.37600255,\n",
       "        0.68262666,  0.3371534 , -1.0618834 ,  0.7140528 ,  0.79375964,\n",
       "       -1.580623  ,  0.08056355,  2.2957795 , -0.4423584 ,  0.85803866,\n",
       "        1.6259893 ,  2.0807257 ,  0.27033195,  0.10869743, -0.7073341 ,\n",
       "        0.93453103,  2.448685  , -1.065508  ,  0.6948831 , -0.3933674 ,\n",
       "       -0.25398833,  0.0525438 ,  0.2760993 ,  0.03046661,  1.6357298 ,\n",
       "       -0.68046504, -0.6552789 ,  0.18674538, -1.5934254 , -1.3352039 ,\n",
       "       -0.11297392,  0.16831501, -3.093571  , -1.304632  , -0.36225113,\n",
       "       -0.85680985, -0.6849744 , -0.6732246 ,  0.7346014 ,  0.04552502,\n",
       "        0.9106011 , -0.8569324 , -0.78676486, -0.7548889 , -0.6239116 ,\n",
       "       -0.10573357,  0.1871993 , -1.0517532 ,  1.2427821 ,  0.09027182,\n",
       "        0.646295  ,  1.4707023 , -1.0996007 ,  0.71657985, -0.4825657 ,\n",
       "        0.7736694 , -0.8214331 , -0.06812207,  0.21072268, -0.05821994,\n",
       "        0.2834152 , -0.5038266 , -0.02220035, -0.7816633 , -0.42758918,\n",
       "       -1.379412  , -0.9991606 , -0.09314307, -1.0375384 ,  1.4078951 ,\n",
       "        0.16383441,  1.1557022 ,  0.6037184 , -0.24036242,  0.6594164 ,\n",
       "       -1.4751513 ,  0.4442909 , -0.10516454,  0.04315067,  0.02415022,\n",
       "       -0.35425392, -0.6597585 ,  0.7272194 , -0.49086562,  0.7478536 ,\n",
       "        0.4731838 ,  1.1517136 ,  1.1910766 ,  0.13328013,  1.2914497 ,\n",
       "       -1.2787122 ,  0.19792289, -1.0412985 , -0.8378643 ,  0.87032825,\n",
       "        1.3638096 ,  0.34298995, -0.78764045, -1.3633085 , -0.6500107 ,\n",
       "        1.8602482 , -0.5194214 , -0.4464755 ,  0.95321774, -0.003745  ,\n",
       "       -0.0477322 , -0.15189315, -0.45838594, -0.51084346,  0.18247887,\n",
       "       -1.0276651 , -1.2960155 , -0.7099005 ,  0.5867346 ,  0.5864896 ,\n",
       "        0.3113188 ,  0.5664665 ,  0.93030953, -0.71497566,  0.86469823,\n",
       "        1.228686  ,  0.20675254,  1.8230547 ,  0.39518136,  0.04141273,\n",
       "        1.2581527 ,  0.69956064, -0.2837691 ,  0.6913725 , -0.0158409 ,\n",
       "       -1.9601034 , -0.6179594 ,  0.7303213 ,  0.445943  , -1.0255207 ,\n",
       "        0.27656153,  0.5854699 , -1.1097488 ,  0.3692454 , -2.777551  ,\n",
       "       -0.03574237, -0.9721121 ,  0.1334582 , -0.4297707 , -0.36648086,\n",
       "       -0.09025213, -0.36026594,  0.09669108,  0.21471927, -1.1047198 ,\n",
       "       -0.4875142 ,  0.5230088 , -1.8361354 ,  1.0928999 , -0.3462372 ,\n",
       "        0.4923618 ,  1.3252195 , -0.29994747,  0.20489064,  1.066884  ,\n",
       "        0.7167484 ,  0.36989203, -0.37181383,  0.51962435, -1.0939481 ,\n",
       "       -0.31082147,  1.6373049 ,  1.3664093 ,  0.5039025 ,  0.7179851 ,\n",
       "        0.08509288, -0.26712167, -0.43728006, -2.5283985 , -0.89836234,\n",
       "       -1.162439  , -1.5820662 , -0.88813573, -0.41403773,  0.17156029,\n",
       "       -0.5983567 ,  0.14165021,  1.9599055 , -0.6432856 ,  0.13173944,\n",
       "        0.13833351,  1.2882144 , -0.26011267,  0.11324546,  0.14307487,\n",
       "        1.6506457 , -0.7683153 ,  0.46653825, -0.0943664 ,  0.06528145],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['computer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardar el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"text8_model\")\n",
    "model=word2vec.Word2Vec.load(\"text8_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cereal'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"breakfast cereal dinner lunch\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'france'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"brazil chile france peru argentina\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hammer'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"apple pear banana hammer\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73985076"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('man','woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31393042"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('man','hammer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20366034"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('woman','hammer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09999105"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('man','engineer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.036601752"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('woman','engineer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.436531"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('man','baby')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56964356"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('woman','baby')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar un modelo Word2Vec pre-entrenado para el español\n",
    "\n",
    "ver: https://github.com/dccuchile/spanish-word-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('./data/SBW-vectors-300-min5.bin.gz', binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "[ 0.1051706  -0.27460352 -0.21322592  0.261666    0.09946854 -0.02449877\n",
      "  0.12955804 -0.34066245  0.3385692  -0.09923615]\n"
     ]
    }
   ],
   "source": [
    "dog = model['perro']\n",
    "print(dog.shape)\n",
    "print(dog[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12951772\n",
      "0.15265214\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('mujer', 'ingeniería'))\n",
    "print(model.similarity('hombre', 'ingenieria'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4396093\n",
      "0.40620732\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('mujer', 'bebe'))\n",
    "print(model.similarity('hombre', 'bebe'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1600553\n",
      "0.20306623\n",
      "0.13652363\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('chileno', 'violencia'))\n",
    "print(model.similarity('venezolano', 'violencia'))\n",
    "print(model.similarity('francés', 'violencia'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17682078\n",
      "0.19584653\n",
      "0.10330092\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('chileno', 'criminal'))\n",
    "print(model.similarity('venezolano', 'criminal'))\n",
    "print(model.similarity('francés', 'criminal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chilena', 0.7700598239898682),\n",
       " ('Chile', 0.7495904564857483),\n",
       " ('peruano', 0.7162680625915527),\n",
       " ('chilenos', 0.7058151960372925),\n",
       " ('Chileno', 0.7000812292098999),\n",
       " ('argentino', 0.6901670694351196),\n",
       " ('boliviano', 0.668327808380127),\n",
       " ('Antofagasta', 0.6591013073921204),\n",
       " ('Talca', 0.6463863849639893),\n",
       " ('ecuatoriano', 0.6427898406982422),\n",
       " ('ariqueño', 0.6422776579856873),\n",
       " ('chilenas', 0.6416400671005249),\n",
       " ('Valparaíso', 0.6410984992980957),\n",
       " ('Pavez', 0.6397318243980408),\n",
       " ('penquista', 0.6381476521492004),\n",
       " ('Iturra', 0.6330534219741821),\n",
       " ('Iquique', 0.6310879588127136),\n",
       " ('nortino', 0.6273276209831238),\n",
       " ('iquiqueño', 0.6271899938583374),\n",
       " ('Temuco', 0.6224685311317444)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"chileno\"],topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitaciones de los word embeddings\n",
    "\n",
    "Las técnicas de Word Embeddings dan resultados muy interesantes pero tienen dos principales limitaciones:\n",
    "\n",
    "1) No permiten tomar en cuenta el orden entre las palabras.\n",
    "\n",
    "Ejemplo: \"Estamos aqui para trabajar y no jugar\" vs. \"Estamos aqui para jugar y no trabajar\"\n",
    "\n",
    "2) No permiten tomar en cuenta que ciertas palabras cambian de significado según el contexto.\n",
    "\n",
    "Ejemplo: \"I lost my computer __mouse__\"\n",
    "\n",
    "Para mejorar estas limitaciones:\n",
    "\n",
    "- Combinar Word Embedding con redes neuronales (convolucionales (CNN) o secuenciales (Transformers, BERT, GPT-3)) que toman en cuenta el orden entre las palabras y el contexto de las palabras.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
