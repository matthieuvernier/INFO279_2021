{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6c436bd",
   "metadata": {},
   "source": [
    "# 1. Introducción: Clasificación de textos utilizando algoritmos de Regresión Logística y Random Forest (sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f343165",
   "metadata": {},
   "source": [
    "- El presente tutorial se inspira de: https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/\n",
    "\n",
    "En este tutorial intentaremos clasificar reseñas de productos Amazon (Alexa) en dos categorías: positivos o negativos (\"Análisis de sentimientos\").\n",
    "\n",
    "Utilizaremos un enfoque simple:\n",
    "- representaremos los textos con representaciones vectoriales \"Bag of Words\"\n",
    "- utilizaremos algoritmos de Machine Learning para aprender modelos a partir de las representaciones vectoriales\n",
    "- evaluaremos los modelos utilizando una matriz de confusión\n",
    "\n",
    "El objetivo principal de este tutorial es entender las limitaciones de este enfoque y por qué se necesitó investigar conceptos más avanzados, como por ejemplo:\n",
    "- \"Word Embeddings / Word2Vec\" (2013): https://arxiv.org/abs/1301.3781\n",
    "- \"Redes neuronales convolucionales para clasificación de textos\" (2014): https://arxiv.org/abs/1408.5882"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1119c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n",
    "#!pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c0a384",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(spacy.__version__)\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "import string\n",
    "\n",
    "#SKLEARN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression # Regresion Logística\n",
    "\n",
    "#PANDAS\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce2ce00",
   "metadata": {},
   "source": [
    "# 2. Dataset: Alexa\n",
    "\n",
    "Vamos a usar un conjunto de datos real: un conjunto de reseñas de productos de Amazon Alexa. Este conjunto de datos viene como un archivo separado por tabulaciones (.tsv). Tiene cinco columnas: \n",
    "- __rating__: se refiere a la calificación que cada usuario dio a Alexa (de 0 a 5). \n",
    "- __fecha__: fecha de la reseña\n",
    "- __variación__: describe el modelo de producto Alexa que el usuario comentó.\n",
    "- __verified_reviews__: contiene el texto del comentario.\n",
    "- __feedback__: contiene un label, 0 o 1, que indica el sentimiento general negativo (0) o positivo (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2235408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading TSV file\n",
    "df_amazon = pd.read_csv(\"../datasets/amazon_alexa.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5f61d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 records\n",
    "df_amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ea77c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of dataframe\n",
    "df_amazon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560613b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feedback Value count\n",
    "df_amazon.feedback.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b450388",
   "metadata": {},
   "source": [
    "#### Partición de los datos en conjuntos de entrenamiento y test para entrenar y evaluar un modelo predictivo\n",
    "\n",
    "Usaremos la mitad de nuestro conjunto de datos como nuestro conjunto de entrenamiento, que incluirá las respuestas correctas. Luego probaremos nuestro modelo usando la otra mitad del conjunto de datos sin darle las respuestas, para ver con qué precisión funciona.\n",
    "\n",
    "Convenientemente, scikit-learn nos da una función incorporada para hacer esto: train_test_split(). Sólo necesitamos decirle el conjunto de características que queremos que se divida (X), las etiquetas contra las que queremos que se realice la prueba (ylabels), y el tamaño que queremos usar para el conjunto de pruebas (representado como un porcentaje en forma decimal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7615f51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_amazon['verified_reviews'] # the features we want to analyze\n",
    "ylabels = df_amazon['feedback'] # the labels, or answers, we want to test against\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06550805",
   "metadata": {},
   "source": [
    "# 3. Preprocesamientos y representación vectorial\n",
    "\n",
    "Crearemos una función personalizada <code>spacy_tokenizer()</code> que acepta una frase como entrada y la procesa en tokens, realizando lemmatización, minúsculas y eliminando palabras stop-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b74ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our list of punctuation marks\n",
    "punctuations = [\".\",\",\",\"!\",\"?\", \"#\",\"&\"]\n",
    "\n",
    "# Create our list of stopwords\n",
    "stop_words=[\"\"]\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = English()\n",
    "\n",
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = parser(sentence)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [word.lower_ for word in mytokens]\n",
    "        \n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39afeae2",
   "metadata": {},
   "source": [
    "#### Vectorización de los textos en BoW o TF-IDF, con scikit-learn\n",
    "\n",
    "Podemos generar una matriz BoW para nuestros datos de texto usando la clase <code>CountVectorizer</code> de scikit-learn. En el código de abajo, le decimos a CountVectorizer que use la función personalizada spacy_tokenizer que construimos como su tokenizer, y que defina el rango de ngramo que queremos.\n",
    "\n",
    "Los N-gramos son combinaciones de palabras adyacentes en un texto dado, donde _n_ es el número de palabras que se incluyen en las fichas. Por ejemplo, en la frase \"¿Quién ganará la Copa del Mundo de fútbol en 2022? Bigramas sería una secuencia de dos palabras contiguas como \"quién ganará\", \"ganará la\", y así sucesivamente. Así que el parámetro ngram_range que usaremos en el código de abajo establece los límites inferior y superior de nuestros ngramas (usaremos unigramas). Entonces asignaremos los ngramas a bow_vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5f634d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))\n",
    "bow_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac7aab7",
   "metadata": {},
   "source": [
    "Podemos también transformar los textos en vectores para tener los pesos TF-IDF de cada palabra en cada documento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080c1433",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0286d177",
   "metadata": {},
   "source": [
    "# 4. Entrenamiento del modelo de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72626c3d",
   "metadata": {},
   "source": [
    "Es el momento de construir nuestro modelo predictivo. Empezaremos importando el módulo LogisticRegression y creando un objeto clasificador LogisticRegression.\n",
    "\n",
    "Luego, crearemos un pipeline de procesamiento con dos componentes: un vectorizador y algoritmo de clasificación basado en la regresión logística. El vectorizador utiliza preprocesamientos (spacy) y vectorización (scikit-learn) para crear una matriz para representar nuestros textos.\n",
    "\n",
    "Una vez que se construya este pipeline, se aprende el modelo predictivo llamando el método fit()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb46765",
   "metadata": {},
   "source": [
    "- Regresión Logística sobre representacion vectorial \"BoW\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07272a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier\n",
    "modelLR = LogisticRegression()\n",
    "\n",
    "# Create pipeline using Bag of Words\n",
    "model1 = Pipeline([('preprocessing', bow_vector),\n",
    "                 ('regression-ML', modelLR)])\n",
    "\n",
    "# model generation\n",
    "model1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64eb9bf",
   "metadata": {},
   "source": [
    "- Regresión Logística sobre representacion vectorial \"TF-IDF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924ce407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier\n",
    "modelLR = LogisticRegression()\n",
    "\n",
    "model2 = Pipeline([('preprocessing', tfidf_vector),\n",
    "                 ('regression-ML', modelLR)])\n",
    "\n",
    "# model generation\n",
    "model2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c1aff0",
   "metadata": {},
   "source": [
    "- Random Forest sobre representacion vectorial \"BoW\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f97bfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "modelRF = RandomForestClassifier(random_state=0)\n",
    "\n",
    "model3 = Pipeline([('preprocessing', bow_vector),\n",
    "                 ('regression-ML', modelRF)])\n",
    "\n",
    "# model generation\n",
    "model3.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96663a86",
   "metadata": {},
   "source": [
    "# 5. Evaluación del modelo de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17e731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting with a test dataset\n",
    "predicted = model1.predict(X_test)\n",
    "print(predicted)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de96cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluación del rendimiento del clasificador\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, predicted)\n",
    "print(confusion_matrix)\n",
    "\n",
    "#Print de la matriz de confusión\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1404e4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printNMostInformative(vectorizer, model, N):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(model.coef_[0], feature_names))\n",
    "    topClass1 = coefs_with_fns[:N]\n",
    "    topClass2 = coefs_with_fns[:-(N + 1):-1]\n",
    "    print(\"Class 1 best: \")\n",
    "    for feat in topClass1:\n",
    "        print(feat)\n",
    "    print(\"Class 2 best: \")\n",
    "    for feat in topClass2:\n",
    "        print(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731a38f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "printNMostInformative(bow_vector, modelLR, 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
